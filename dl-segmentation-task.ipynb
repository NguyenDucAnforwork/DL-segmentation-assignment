{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10000713,"sourceType":"datasetVersion","datasetId":6155660}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T14:46:45.514271Z","iopub.execute_input":"2024-11-24T14:46:45.514740Z","iopub.status.idle":"2024-11-24T14:46:45.519916Z","shell.execute_reply.started":"2024-11-24T14:46:45.514708Z","shell.execute_reply":"2024-11-24T14:46:45.519004Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install torchsummary torchgeometry segmentation-models-pytorch tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T14:51:05.128083Z","iopub.execute_input":"2024-11-24T14:51:05.129069Z","iopub.status.idle":"2024-11-24T14:51:19.962210Z","shell.execute_reply.started":"2024-11-24T14:51:05.129021Z","shell.execute_reply":"2024-11-24T14:51:19.961407Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nCollecting torchgeometry\n  Downloading torchgeometry-0.1.2-py2.py3-none-any.whl.metadata (2.9 kB)\nCollecting segmentation-models-pytorch\n  Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from torchgeometry) (2.4.0)\nCollecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: huggingface-hub>=0.24.6 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.25.1)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (10.3.0)\nCollecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (1.16.0)\nCollecting timm==0.9.7 (from segmentation-models-pytorch)\n  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.19.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (4.66.4)\nCollecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.7->segmentation-models-pytorch) (6.0.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.7->segmentation-models-pytorch) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.1.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.26.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->torchgeometry) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->torchgeometry) (1.3.0)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nDownloading torchgeometry-0.1.2-py2.py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading segmentation_models_pytorch-0.3.4-py3-none-any.whl (109 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16427 sha256=c44f5871045b6e4e01621a6f14aff409f39255df0c06154cee35b253b85a2401\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=65a254e60814b520cb5c7bfcdf4ae2559751b01dd1d1e25c99fc23fe522ab2c6\n  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\nSuccessfully built efficientnet-pytorch pretrainedmodels\nInstalling collected packages: torchsummary, munch, torchgeometry, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n  Attempting uninstall: timm\n    Found existing installation: timm 1.0.9\n    Uninstalling timm-1.0.9:\n      Successfully uninstalled timm-1.0.9\nSuccessfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.4 timm-0.9.7 torchgeometry-0.1.2 torchsummary-1.5.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T15:01:28.298374Z","iopub.execute_input":"2024-11-24T15:01:28.298717Z","iopub.status.idle":"2024-11-24T15:01:36.542057Z","shell.execute_reply.started":"2024-11-24T15:01:28.298689Z","shell.execute_reply":"2024-11-24T15:01:36.540912Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# import libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport cv2\nfrom torchvision.io import read_image\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, random_split, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nfrom torchvision.transforms import ToTensor\nfrom PIL import Image\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision \nfrom torchvision import transforms\nfrom torchinfo import summary\nimport timm\nimport segmentation_models_pytorch as smp\nimport wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T14:51:35.504016Z","iopub.execute_input":"2024-11-24T14:51:35.504643Z","iopub.status.idle":"2024-11-24T14:51:42.784101Z","shell.execute_reply.started":"2024-11-24T14:51:35.504604Z","shell.execute_reply":"2024-11-24T14:51:42.783404Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.17). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# configuration\nBATCH_SIZE = 8\nlr = 0.01\nNUM_EPOCHS = 50","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T15:04:16.483900Z","iopub.execute_input":"2024-11-24T15:04:16.484248Z","iopub.status.idle":"2024-11-24T15:04:16.489212Z","shell.execute_reply.started":"2024-11-24T15:04:16.484220Z","shell.execute_reply":"2024-11-24T15:04:16.488467Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# dataset\nclass DatasetCustom(Dataset):\n    def __init__(self, img_dir, label_dir, resize=None, transform=None):\n        self.img_dir = img_dir\n        self.label_dir = label_dir\n        self.resize = resize\n        self.transform = transform\n        self.images = os.listdir(self.img_dir)\n\n    def __len__(self):\n        return len(self.images)\n    \n    def read_mask(self, mask_path):\n        image = cv2.imread(mask_path)\n        image = cv2.resize(image, self.resize)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        lower_red1 = np.array([0, 100, 20])\n        upper_red1 = np.array([10, 255, 255])\n        lower_red2 = np.array([160,100,20])\n        upper_red2 = np.array([179,255,255])\n        \n        lower_mask_red = cv2.inRange(image, lower_red1, upper_red1)\n        upper_mask_red = cv2.inRange(image, lower_red2, upper_red2)\n        \n        red_mask = lower_mask_red + upper_mask_red\n        red_mask[red_mask != 0] = 1\n\n        green_mask = cv2.inRange(image, (36, 25, 25), (70, 255, 255))\n        green_mask[green_mask != 0] = 2\n\n        full_mask = cv2.bitwise_or(red_mask, green_mask)\n        full_mask = np.expand_dims(full_mask, axis=-1) \n        full_mask = full_mask.astype(np.uint8)\n        \n        return full_mask\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.images[idx])\n        label_path = os.path.join(self.label_dir, self.images[idx])\n        image = cv2.imread(img_path)  #  BGR\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Convert to RGB\n        label = self.read_mask(label_path)  \n        image = cv2.resize(image, self.resize)\n        if self.transform:\n            image = self.transform(image)\n            \n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T14:52:34.150307Z","iopub.execute_input":"2024-11-24T14:52:34.150647Z","iopub.status.idle":"2024-11-24T14:52:34.160200Z","shell.execute_reply.started":"2024-11-24T14:52:34.150618Z","shell.execute_reply":"2024-11-24T14:52:34.159411Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"cd ..","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T14:53:34.174911Z","iopub.execute_input":"2024-11-24T14:53:34.175755Z","iopub.status.idle":"2024-11-24T14:53:34.180808Z","shell.execute_reply.started":"2024-11-24T14:53:34.175723Z","shell.execute_reply":"2024-11-24T14:53:34.179772Z"}},"outputs":[{"name":"stdout","text":"/\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# load images + masks\nimages_path = \"/kaggle/input/polyp-dataset/train/train/\"\nimage_path = []\nTRAIN_DIR = '/kaggle/input/polyp-dataset/train/train'\nfor root, dirs, files in os.walk(TRAIN_DIR):\n    for file in files:\n        path = os.path.join(root,file)\n        image_path.append(path)\n        \nprint(len(image_path))\n\nmask_path = []\nTRAIN_MASK_DIR = '/kaggle/input/polyp-dataset/train_gt/train_gt'\nfor root, dirs, files in os.walk(TRAIN_MASK_DIR):\n    for file in files:\n        path = os.path.join(root,file)\n        mask_path.append(path)\n        \nprint(len(mask_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T14:54:06.373217Z","iopub.execute_input":"2024-11-24T14:54:06.373564Z","iopub.status.idle":"2024-11-24T14:54:07.265213Z","shell.execute_reply.started":"2024-11-24T14:54:06.373535Z","shell.execute_reply":"2024-11-24T14:54:07.264245Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"1000\n1000\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# store images + masks\nimages_data = []\nmasks_data = []\nfor x,y in dataset:\n    images_data.append(x)\n    masks_data.append(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T14:56:15.251370Z","iopub.execute_input":"2024-11-24T14:56:15.251725Z","iopub.status.idle":"2024-11-24T14:56:42.079758Z","shell.execute_reply.started":"2024-11-24T14:56:15.251695Z","shell.execute_reply":"2024-11-24T14:56:42.079018Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"len(images_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T14:56:53.107577Z","iopub.execute_input":"2024-11-24T14:56:53.108222Z","iopub.status.idle":"2024-11-24T14:56:53.114205Z","shell.execute_reply.started":"2024-11-24T14:56:53.108191Z","shell.execute_reply":"2024-11-24T14:56:53.113246Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"1000"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# define train + val transformation\ntrain_transformation = A.Compose([\n    A.HorizontalFlip(p=0.4),\n    A.VerticalFlip(p=0.4),\n    A.RandomGamma (gamma_limit=(70, 130), eps=None, always_apply=False, p=0.2),\n    A.RGBShift(p=0.3, r_shift_limit=10, g_shift_limit=10, b_shift_limit=10),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n\nval_transformation = A.Compose([\n    A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T14:55:18.534415Z","iopub.execute_input":"2024-11-24T14:55:18.534767Z","iopub.status.idle":"2024-11-24T14:55:18.542706Z","shell.execute_reply.started":"2024-11-24T14:55:18.534737Z","shell.execute_reply":"2024-11-24T14:55:18.541891Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3580890972.py:5: UserWarning: Argument 'eps' is not valid and will be ignored.\n  A.RandomGamma (gamma_limit=(70, 130), eps=None, always_apply=False, p=0.2),\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# CustomDataset\nclass CustomDataset(Dataset):\n    def __init__(self, data, targets, transform=None):\n        self.data = data\n        self.targets = targets\n        self.transform = transform\n\n    def __getitem__(self, index):\n        image = self.data[index]\n        label = self.targets[index]\n        assert image.shape[:2] == label.shape[:2]\n        if self.transform:\n            transformed = self.transform(image=image, mask=label)\n            image = transformed['image'].float()\n            label = transformed['mask'].float()\n            label = label.permute(2, 0, 1)\n        return image, label\n    \n    def __len__(self):\n        return len(self.data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T14:59:02.804963Z","iopub.execute_input":"2024-11-24T14:59:02.805825Z","iopub.status.idle":"2024-11-24T14:59:02.811854Z","shell.execute_reply.started":"2024-11-24T14:59:02.805793Z","shell.execute_reply":"2024-11-24T14:59:02.810897Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# split train and eval set\ntrain_size = int(0.8 * len(images_data))\nval_size = len(images_data) - train_size\ntrain_dataset = CustomDataset(images_data[:train_size], masks_data[:train_size], transform=train_transformation)\nval_dataset = CustomDataset(images_data[train_size:], masks_data[train_size:], transform=val_transformation)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T14:59:41.817005Z","iopub.execute_input":"2024-11-24T14:59:41.817330Z","iopub.status.idle":"2024-11-24T14:59:41.822822Z","shell.execute_reply.started":"2024-11-24T14:59:41.817305Z","shell.execute_reply":"2024-11-24T14:59:41.821888Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# model\nimport segmentation_models_pytorch as smp\n\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"resnet34\",        \n    encoder_weights=\"imagenet\",     \n    in_channels=3,                  \n    classes=3     \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T15:01:10.132965Z","iopub.execute_input":"2024-11-24T15:01:10.133302Z","iopub.status.idle":"2024-11-24T15:01:10.803703Z","shell.execute_reply.started":"2024-11-24T15:01:10.133253Z","shell.execute_reply":"2024-11-24T15:01:10.802750Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# login to wandb\nwandb.login(\n    # set the wandb project where this run will be logged\n#     project= \"PolypSegment\", \n    key = \"e8c9ee40b64158f0b2cbede51f3751b64d9368e4\",\n)\nwandb.init(\n    project = \"PolypSegment\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T15:03:46.955373Z","iopub.execute_input":"2024-11-24T15:03:46.956238Z","iopub.status.idle":"2024-11-24T15:03:51.626334Z","shell.execute_reply.started":"2024-11-24T15:03:46.956198Z","shell.execute_reply":"2024-11-24T15:03:51.625488Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33man-nd225432\u001b[0m (\u001b[33man-nd225432-hust\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111372004444446, max=1.0)…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8c47223cdda4a2ba9e0e52af3ef34a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/wandb/run-20241124_150348-ztfxsyws</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/an-nd225432-hust/PolypSegment/runs/ztfxsyws' target=\"_blank\">zany-bee-1</a></strong> to <a href='https://wandb.ai/an-nd225432-hust/PolypSegment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/an-nd225432-hust/PolypSegment' target=\"_blank\">https://wandb.ai/an-nd225432-hust/PolypSegment</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/an-nd225432-hust/PolypSegment/runs/ztfxsyws' target=\"_blank\">https://wandb.ai/an-nd225432-hust/PolypSegment/runs/ztfxsyws</a>"},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/an-nd225432-hust/PolypSegment/runs/ztfxsyws?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x781a247cb370>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=lr)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T15:06:01.620962Z","iopub.execute_input":"2024-11-24T15:06:01.621756Z","iopub.status.idle":"2024-11-24T15:06:01.628465Z","shell.execute_reply.started":"2024-11-24T15:06:01.621722Z","shell.execute_reply":"2024-11-24T15:06:01.627455Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # Decays LR every 10 epochs by 0.1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:26:25.768605Z","iopub.execute_input":"2024-11-24T16:26:25.769270Z","iopub.status.idle":"2024-11-24T16:26:25.774568Z","shell.execute_reply.started":"2024-11-24T16:26:25.769237Z","shell.execute_reply":"2024-11-24T16:26:25.773611Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"from tqdm import tqdm\nimport time\n\nNUM_EPOCHS = 30\ndevice = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\ncriterion = nn.CrossEntropyLoss()\nbest_val_loss = 999\n\nepoch_bar = tqdm(total=NUM_EPOCHS, desc='Total Progress')\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    train_loss = 0\n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        labels = labels.squeeze(dim=1).long()\n        outputs = model(images)\n    \n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        train_loss += loss.item()\n    \n    model.eval()\n    with torch.no_grad():\n        val_loss = 0\n        for images, labels in val_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            labels = labels.squeeze(dim=1).long()\n            \n            outputs = model(images)\n\n            val_loss += criterion(outputs.float(),labels.long()).item()\n\n    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {val_loss/len(val_loader):.10f}\")\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        checkpoint = { \n            'epoch': epoch,\n            'model': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n            'loss': val_loss,\n        }\n        save_path = f'model.pth'\n        torch.save(checkpoint, save_path)\n\n    # Step the scheduler\n    scheduler.step()\n        \n    epoch_bar.update(1)\n    wandb.log({'Val_loss': val_loss/len(val_loader),'Train_loss': train_loss/len(train_loader)})\nepoch_bar.close()\n    \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:27:18.546732Z","iopub.execute_input":"2024-11-24T16:27:18.547100Z","iopub.status.idle":"2024-11-24T16:38:28.743277Z","shell.execute_reply.started":"2024-11-24T16:27:18.547069Z","shell.execute_reply":"2024-11-24T16:38:28.742025Z"}},"outputs":[{"name":"stderr","text":"\nTotal Progress:  20%|██        | 10/50 [06:39<26:37, 39.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/30], Loss: 0.0889906291\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:   3%|▎         | 1/30 [00:25<12:24, 25.66s/it]\u001b[A\nTotal Progress:   7%|▋         | 2/30 [00:50<11:49, 25.33s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [2/30], Loss: 0.0900403210\nEpoch [3/30], Loss: 0.0851921190\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  10%|█         | 3/30 [01:16<11:34, 25.73s/it]\u001b[A\nTotal Progress:  13%|█▎        | 4/30 [01:42<11:05, 25.61s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [4/30], Loss: 0.0927702053\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  17%|█▋        | 5/30 [02:07<10:39, 25.59s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [5/30], Loss: 0.0852859908\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  20%|██        | 6/30 [02:33<10:14, 25.61s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [6/30], Loss: 0.1112999174\nEpoch [7/30], Loss: 0.0845485805\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  23%|██▎       | 7/30 [03:00<09:55, 25.90s/it]\u001b[A\nTotal Progress:  27%|██▋       | 8/30 [03:25<09:29, 25.87s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [8/30], Loss: 0.0846036845\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  30%|███       | 9/30 [03:51<09:03, 25.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [9/30], Loss: 0.0942606836\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  33%|███▎      | 10/30 [04:17<08:37, 25.86s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [10/30], Loss: 0.0878339079\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  37%|███▋      | 11/30 [04:43<08:11, 25.89s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [11/30], Loss: 0.0884225827\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  40%|████      | 12/30 [05:09<07:46, 25.91s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [12/30], Loss: 0.0954486979\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  43%|████▎     | 13/30 [05:35<07:20, 25.92s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [13/30], Loss: 0.0980502082\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  47%|████▋     | 14/30 [06:01<06:55, 25.94s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [14/30], Loss: 0.0975148846\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  50%|█████     | 15/30 [06:27<06:29, 25.96s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [15/30], Loss: 0.0970253110\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  53%|█████▎    | 16/30 [06:53<06:03, 25.97s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [16/30], Loss: 0.1002762566\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  57%|█████▋    | 17/30 [07:19<05:37, 25.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [17/30], Loss: 0.1001070603\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  60%|██████    | 18/30 [07:45<05:11, 25.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [18/30], Loss: 0.1018136478\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  63%|██████▎   | 19/30 [08:11<04:45, 25.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [19/30], Loss: 0.1023111297\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  67%|██████▋   | 20/30 [08:37<04:19, 25.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [20/30], Loss: 0.1022345591\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  70%|███████   | 21/30 [09:03<03:53, 25.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [21/30], Loss: 0.1025553986\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  73%|███████▎  | 22/30 [09:29<03:27, 25.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [22/30], Loss: 0.1034216625\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  77%|███████▋  | 23/30 [09:55<03:01, 25.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [23/30], Loss: 0.1055805768\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  80%|████████  | 24/30 [10:21<02:35, 25.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [24/30], Loss: 0.1076800600\n","output_type":"stream"},{"name":"stderr","text":"\nTotal Progress:  83%|████████▎ | 25/30 [10:47<02:09, 25.98s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch [25/30], Loss: 0.1051756456\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[70], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     14\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     16\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[18], line 13\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m image\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m label\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 13\u001b[0m     transformed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     image \u001b[38;5;241m=\u001b[39m transformed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     15\u001b[0m     label \u001b[38;5;241m=\u001b[39m transformed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/albumentations/core/composition.py:348\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(data)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m--> 348\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_data_post_transform(data)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(data)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/albumentations/core/transforms_interface.py:124\u001b[0m, in \u001b[0;36mBasicTransform.__call__\u001b[0;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterministic:\n\u001b[1;32m    123\u001b[0m         kwargs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_key][\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] \u001b[38;5;241m=\u001b[39m deepcopy(params)\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_with_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kwargs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/albumentations/core/transforms_interface.py:143\u001b[0m, in \u001b[0;36mBasicTransform.apply_with_params\u001b[0;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m target_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key2func[key]\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 143\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequirements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC_CONTIGUOUS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    145\u001b[0m         res[key] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrequire(result, requirements\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC_CONTIGUOUS\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/albumentations/augmentations/transforms.py:324\u001b[0m, in \u001b[0;36mNormalize.apply\u001b[0;34m(self, img, **params)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, img: np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalization \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_np\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdenominator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m normalize_per_image(img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalization)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/albucore/functions.py:259\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(img, mean, denominator)\u001b[0m\n\u001b[1;32m    257\u001b[0m mean \u001b[38;5;241m=\u001b[39m convert_value(mean, num_channels)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39muint8:\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnormalize_lut\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenominator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m normalize_opencv(img, mean, denominator)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/albucore/decorators.py:43\u001b[0m, in \u001b[0;36mpreserve_channel_dim.<locals>.wrapped_function\u001b[0;34m(img, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_function\u001b[39m(img: np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     42\u001b[0m     shape \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 43\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shape) \u001b[38;5;241m==\u001b[39m NUM_MULTI_CHANNEL_DIMENSIONS \u001b[38;5;129;01mand\u001b[39;00m shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m MONO_CHANNEL_DIMENSIONS:\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mexpand_dims(result, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/albucore/functions.py:251\u001b[0m, in \u001b[0;36mnormalize_lut\u001b[0;34m(img, mean, denominator)\u001b[0m\n\u001b[1;32m    247\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    249\u001b[0m luts \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, max_value \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m*\u001b[39m denominator\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLUT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mluts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_channels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":70},{"cell_type":"code","source":"torch.save(checkpoint, f'./kaggle/working/model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:45:15.931712Z","iopub.execute_input":"2024-11-24T16:45:15.932201Z","iopub.status.idle":"2024-11-24T16:45:16.452420Z","shell.execute_reply.started":"2024-11-24T16:45:15.932161Z","shell.execute_reply":"2024-11-24T16:45:16.451701Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"!mkdir prediction\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T15:35:15.537639Z","iopub.execute_input":"2024-11-24T15:35:15.538001Z","iopub.status.idle":"2024-11-24T15:35:16.587118Z","shell.execute_reply.started":"2024-11-24T15:35:15.537972Z","shell.execute_reply":"2024-11-24T15:35:16.585933Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"cd ..","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T15:35:43.639015Z","iopub.execute_input":"2024-11-24T15:35:43.639355Z","iopub.status.idle":"2024-11-24T15:35:43.646175Z","shell.execute_reply.started":"2024-11-24T15:35:43.639326Z","shell.execute_reply":"2024-11-24T15:35:43.645320Z"}},"outputs":[{"name":"stdout","text":"/\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# mask to rgb\ncolor_dict= {0: (0, 0, 0),\n             1: (255, 0, 0),\n             2: (0, 255, 0)}\ndef mask_to_rgb(mask, color_dict):\n    output = np.zeros((mask.shape[0], mask.shape[1], 3))\n\n    for k in color_dict.keys():\n        output[mask==k] = color_dict[k]\n\n    return np.uint8(output)   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T15:36:26.232980Z","iopub.execute_input":"2024-11-24T15:36:26.233677Z","iopub.status.idle":"2024-11-24T15:36:26.239968Z","shell.execute_reply.started":"2024-11-24T15:36:26.233645Z","shell.execute_reply":"2024-11-24T15:36:26.239147Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# transforms for evaluation\nfrom albumentations import Compose, Resize, Normalize\n\ntransforms = Compose([\n    Resize(512, 512),  # Resize images to 512x512\n    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize images\n    ToTensorV2()  # Convert to PyTorch tensors, (C, H, W)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T15:46:41.918993Z","iopub.execute_input":"2024-11-24T15:46:41.919350Z","iopub.status.idle":"2024-11-24T15:46:41.926512Z","shell.execute_reply.started":"2024-11-24T15:46:41.919320Z","shell.execute_reply":"2024-11-24T15:46:41.925586Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"model.eval()\noutput_dir = \"./kaggle/working/prediction\"\n\nfor i in os.listdir(\"/kaggle/input/polyp-dataset/test/test\"):\n    img_path = os.path.join(\"/kaggle/input/polyp-dataset/test/test\", i)\n    ori_img = cv2.imread(img_path)\n    ori_img = cv2.cvtColor(ori_img, cv2.COLOR_BGR2RGB)\n    ori_w = ori_img.shape[0]\n    ori_h = ori_img.shape[1]\n    img = cv2.resize(ori_img, (256, 256))\n    transformed = val_transformation(image=img)\n    input_img = transformed[\"image\"]\n    input_img = input_img.unsqueeze(0).to(device)\n    with torch.no_grad():\n        output_mask = model.forward(input_img).squeeze(0).cpu().numpy().transpose(1,2,0)\n        # print(output_mask.shape)\n    mask = cv2.resize(output_mask, (ori_h, ori_w))\n    mask = np.argmax(mask, axis=2)\n    mask_rgb = mask_to_rgb(mask, color_dict)\n    mask_rgb = cv2.cvtColor(mask_rgb, cv2.COLOR_RGB2BGR)\n    output_path = os.path.join(output_dir, i)\n    cv2.imwrite(output_path, mask_rgb)\n    print(f\"Saved: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:38:47.079807Z","iopub.execute_input":"2024-11-24T16:38:47.080469Z","iopub.status.idle":"2024-11-24T16:39:06.048393Z","shell.execute_reply.started":"2024-11-24T16:38:47.080432Z","shell.execute_reply":"2024-11-24T16:39:06.047541Z"}},"outputs":[{"name":"stdout","text":"Saved: ./kaggle/working/prediction/f62f215f0da4ad3a7ab8df9da7386835.jpeg\nSaved: ./kaggle/working/prediction/aeeb2b535797395305af926a6f23c5d6.jpeg\nSaved: ./kaggle/working/prediction/2ed9fbb63b28163a745959c03983064a.jpeg\nSaved: ./kaggle/working/prediction/3c84417fda8019410b1fcf0625f608b4.jpeg\nSaved: ./kaggle/working/prediction/8fa8625605da2023387fd56c04414eaa.jpeg\nSaved: ./kaggle/working/prediction/cb1b387133b51209db6dcdda5cc8a788.jpeg\nSaved: ./kaggle/working/prediction/a3657e4314fe384eb2ba3adfda6c1899.jpeg\nSaved: ./kaggle/working/prediction/c695325ded465efde988dfb96d081533.jpeg\nSaved: ./kaggle/working/prediction/0fca6a4248a41e8db8b4ed633b456aaa.jpeg\nSaved: ./kaggle/working/prediction/6f4d4987ea3b4bae5672a230194c5a08.jpeg\nSaved: ./kaggle/working/prediction/68d4b4ef4d95ceea11957998906d3694.jpeg\nSaved: ./kaggle/working/prediction/559c7e610b1531871f2fd85a04faeeb2.jpeg\nSaved: ./kaggle/working/prediction/e5e8f14e1e0ae936de314f2d95e6c487.jpeg\nSaved: ./kaggle/working/prediction/3c3ca4d5060a633a8d5b2b2b55157b77.jpeg\nSaved: ./kaggle/working/prediction/6d3694abb47953b0e4909384b57bb6a0.jpeg\nSaved: ./kaggle/working/prediction/c7e610b1531871f2fd85a04faeeb2b53.jpeg\nSaved: ./kaggle/working/prediction/7cdf3f33c3ca4d5060a633a8d5b2b2b5.jpeg\nSaved: ./kaggle/working/prediction/e2cd066b9fdbc3bbc04a3afe1f119f21.jpeg\nSaved: ./kaggle/working/prediction/b21960c94b0aab4c024a573c692195f8.jpeg\nSaved: ./kaggle/working/prediction/cb2eb1ef57af2ed9fbb63b28163a7459.jpeg\nSaved: ./kaggle/working/prediction/3bbc04a3afe1f119f21b248d152b672a.jpeg\nSaved: ./kaggle/working/prediction/e1e0ae936de314f2d95e6c487ffa651b.jpeg\nSaved: ./kaggle/working/prediction/d077bad31c8c5f54ffaa27a623511c38.jpeg\nSaved: ./kaggle/working/prediction/2cd066b9fdbc3bbc04a3afe1f119f21b.jpeg\nSaved: ./kaggle/working/prediction/e19769fa2d37d32780fd497e1c0e9082.jpeg\nSaved: ./kaggle/working/prediction/af35b65bd9ea42cfcfedb5eb2a0e4b50.jpeg\nSaved: ./kaggle/working/prediction/26679bff55177a34fc01019eec999fd8.jpeg\nSaved: ./kaggle/working/prediction/8cbdf366e057db382b8564872a27301a.jpeg\nSaved: ./kaggle/working/prediction/e1797c77826f9a7021bab9fc73303988.jpeg\nSaved: ./kaggle/working/prediction/d694539ef2424a9218697283baa3657e.jpeg\nSaved: ./kaggle/working/prediction/eecd70ebce6347c491b37c8c2e5a64a8.jpeg\nSaved: ./kaggle/working/prediction/d6bf62f215f0da4ad3a7ab8df9da7386.jpeg\nSaved: ./kaggle/working/prediction/5b21960c94b0aab4c024a573c692195f.jpeg\nSaved: ./kaggle/working/prediction/5a51625559c7e610b1531871f2fd85a0.jpeg\nSaved: ./kaggle/working/prediction/3657e4314fe384eb2ba3adfda6c1899f.jpeg\nSaved: ./kaggle/working/prediction/7ad1cf2eb9d32a3dc907950289e976c7.jpeg\nSaved: ./kaggle/working/prediction/677a6b1f2c6d40b3bbba8f6c704801b3.jpeg\nSaved: ./kaggle/working/prediction/bec33b5e3d68f9d4c331587f9b9d49e2.jpeg\nSaved: ./kaggle/working/prediction/c22268d4b4ef4d95ceea11957998906d.jpeg\nSaved: ./kaggle/working/prediction/be86f03d900fd197cd955fa095f97845.jpeg\nSaved: ./kaggle/working/prediction/391adc0bb223c4eaf3372eae567c94ea.jpeg\nSaved: ./kaggle/working/prediction/f7fdb2d45b21960c94b0aab4c024a573.jpeg\nSaved: ./kaggle/working/prediction/a51625559c7e610b1531871f2fd85a04.jpeg\nSaved: ./kaggle/working/prediction/461c2a337948a41964c1d4f50a5f3601.jpeg\nSaved: ./kaggle/working/prediction/1ad4f13ccf1f4b331a412fc44655fb51.jpeg\nSaved: ./kaggle/working/prediction/dd78294679c9cbb2a365b5574868eb60.jpeg\nSaved: ./kaggle/working/prediction/54ba59c7de13a35276a476420655433a.jpeg\nSaved: ./kaggle/working/prediction/77e004e8bfb905b78a91391adc0bb223.jpeg\nSaved: ./kaggle/working/prediction/e3c84417fda8019410b1fcf0625f608b.jpeg\nSaved: ./kaggle/working/prediction/97e1c0e9082ea2c193ac8d551c149b60.jpeg\nSaved: ./kaggle/working/prediction/3c692195f853af7f8a4df1ec859759b7.jpeg\nSaved: ./kaggle/working/prediction/cdf3f33c3ca4d5060a633a8d5b2b2b55.jpeg\nSaved: ./kaggle/working/prediction/7af2ed9fbb63b28163a745959c039830.jpeg\nSaved: ./kaggle/working/prediction/5e8f14e1e0ae936de314f2d95e6c487f.jpeg\nSaved: ./kaggle/working/prediction/afe1f119f21b248d152b672ab3492fc6.jpeg\nSaved: ./kaggle/working/prediction/b70dd094a7f32574d6c748c41743c6c0.jpeg\nSaved: ./kaggle/working/prediction/ff55177a34fc01019eec999fd84e679b.jpeg\nSaved: ./kaggle/working/prediction/fb905b78a91391adc0bb223c4eaf3372.jpeg\nSaved: ./kaggle/working/prediction/6ddca6ee1af35b65bd9ea42cfcfedb5e.jpeg\nSaved: ./kaggle/working/prediction/87133b51209db6dcdda5cc8a788edaeb.jpeg\nSaved: ./kaggle/working/prediction/7936140a2d5fc1443c4e445927738677.jpeg\nSaved: ./kaggle/working/prediction/82ea2c193ac8d551c149b60f2965341c.jpeg\nSaved: ./kaggle/working/prediction/3dd311a65d2b46d0a6085835c525af63.jpeg\nSaved: ./kaggle/working/prediction/3f33c3ca4d5060a633a8d5b2b2b55157.jpeg\nSaved: ./kaggle/working/prediction/7f0019f7e6af7d7147763bdfb928d788.jpeg\nSaved: ./kaggle/working/prediction/9c7976c1182df0de51d32128c358d1fd.jpeg\nSaved: ./kaggle/working/prediction/a9d45c3dbc695325ded465efde988dfb.jpeg\nSaved: ./kaggle/working/prediction/ea42b4eebc9e5a87e443434ac60af150.jpeg\nSaved: ./kaggle/working/prediction/67d4dcf9596154efb7cef748d9cbd617.jpeg\nSaved: ./kaggle/working/prediction/625559c7e610b1531871f2fd85a04fae.jpeg\nSaved: ./kaggle/working/prediction/1b62f15ec83b97bb11e8e0c4416c1931.jpeg\nSaved: ./kaggle/working/prediction/780fd497e1c0e9082ea2c193ac8d551c.jpeg\nSaved: ./kaggle/working/prediction/a6d9ba9d45c3dbc695325ded465efde9.jpeg\nSaved: ./kaggle/working/prediction/f14e1e0ae936de314f2d95e6c487ffa6.jpeg\nSaved: ./kaggle/working/prediction/a15fc656702fa602bb3c7abacdbd7e6a.jpeg\nSaved: ./kaggle/working/prediction/dd094a7f32574d6c748c41743c6c08a1.jpeg\nSaved: ./kaggle/working/prediction/314fe384eb2ba3adfda6c1899fdc9837.jpeg\nSaved: ./kaggle/working/prediction/6b83ef461c2a337948a41964c1d4f50a.jpeg\nSaved: ./kaggle/working/prediction/88e16d4ca6160127cd1d5ff99c267599.jpeg\nSaved: ./kaggle/working/prediction/1209db6dcdda5cc8a788edaeb6aa460a.jpeg\nSaved: ./kaggle/working/prediction/f8e26031fbb5e52c41545ba55aadaa77.jpeg\nSaved: ./kaggle/working/prediction/aafac813fe3ccba3e032dd2948a80c64.jpeg\nSaved: ./kaggle/working/prediction/7cb2eb1ef57af2ed9fbb63b28163a745.jpeg\nSaved: ./kaggle/working/prediction/cbb2a365b5574868eb60861ee1ff0b8a.jpeg\nSaved: ./kaggle/working/prediction/6679bff55177a34fc01019eec999fd84.jpeg\nSaved: ./kaggle/working/prediction/6231002ec4a1fe748f3085f1ce88cbdf.jpeg\nSaved: ./kaggle/working/prediction/e73749a0d21db70dd094a7f32574d6c7.jpeg\nSaved: ./kaggle/working/prediction/05b78a91391adc0bb223c4eaf3372eae.jpeg\nSaved: ./kaggle/working/prediction/c193ac8d551c149b60f2965341caf528.jpeg\nSaved: ./kaggle/working/prediction/e56a6d9ba9d45c3dbc695325ded465ef.jpeg\nSaved: ./kaggle/working/prediction/4417fda8019410b1fcf0625f608b4ce9.jpeg\nSaved: ./kaggle/working/prediction/2a365b5574868eb60861ee1ff0b8a4f6.jpeg\nSaved: ./kaggle/working/prediction/0398846f67b5df7cdf3f33c3ca4d5060.jpeg\nSaved: ./kaggle/working/prediction/a48847ae8395e56a6d9ba9d45c3dbc69.jpeg\nSaved: ./kaggle/working/prediction/63b8318ecf467d7ad048df39beb17636.jpeg\nSaved: ./kaggle/working/prediction/0a0317371a966bf4b3466463a3c64db1.jpeg\nSaved: ./kaggle/working/prediction/4ef4d95ceea11957998906d3694abb47.jpeg\nSaved: ./kaggle/working/prediction/80cae6daedd989517cb8041ed86e5822.jpeg\nSaved: ./kaggle/working/prediction/fdbc3bbc04a3afe1f119f21b248d152b.jpeg\nSaved: ./kaggle/working/prediction/1002ec4a1fe748f3085f1ce88cbdf366.jpeg\nSaved: ./kaggle/working/prediction/626650908b1cb932a767bf5487ced51b.jpeg\nSaved: ./kaggle/working/prediction/85a04faeeb2b535797395305af926a6f.jpeg\nSaved: ./kaggle/working/prediction/72d9e593b6be1ac29adbe86f03d900fd.jpeg\nSaved: ./kaggle/working/prediction/94a7f32574d6c748c41743c6c08a1d1a.jpeg\nSaved: ./kaggle/working/prediction/710d568df17586ad8f3297c819c90895.jpeg\nSaved: ./kaggle/working/prediction/c4be73749a0d21db70dd094a7f32574d.jpeg\nSaved: ./kaggle/working/prediction/45b21960c94b0aab4c024a573c692195.jpeg\nSaved: ./kaggle/working/prediction/4e2a6e51d077bad31c8c5f54ffaa27a6.jpeg\nSaved: ./kaggle/working/prediction/0af3feff05dec1eb3a70b145a7d8d3b6.jpeg\nSaved: ./kaggle/working/prediction/1db239dda50f954ba59c7de13a35276a.jpeg\nSaved: ./kaggle/working/prediction/2d9e593b6be1ac29adbe86f03d900fd1.jpeg\nSaved: ./kaggle/working/prediction/df8e26031fbb5e52c41545ba55aadaa7.jpeg\nSaved: ./kaggle/working/prediction/0a5f3601ad4f13ccf1f4b331a412fc44.jpeg\nSaved: ./kaggle/working/prediction/4ca6160127cd1d5ff99c267599fc487b.jpeg\nSaved: ./kaggle/working/prediction/c656702fa602bb3c7abacdbd7e6afd56.jpeg\nSaved: ./kaggle/working/prediction/02fa602bb3c7abacdbd7e6afd56ea7bc.jpeg\nSaved: ./kaggle/working/prediction/343f27ebc5d92b9076135d76d0bbd4ce.jpeg\nSaved: ./kaggle/working/prediction/318ecf467d7ad048df39beb176363408.jpeg\nSaved: ./kaggle/working/prediction/9fc7330398846f67b5df7cdf3f33c3ca.jpeg\nSaved: ./kaggle/working/prediction/66e057db382b8564872a27301a654864.jpeg\nSaved: ./kaggle/working/prediction/ca4d5060a633a8d5b2b2b55157b7781e.jpeg\nSaved: ./kaggle/working/prediction/a6e51d077bad31c8c5f54ffaa27a6235.jpeg\nSaved: ./kaggle/working/prediction/e9082ea2c193ac8d551c149b60f29653.jpeg\nSaved: ./kaggle/working/prediction/6ad1468996b4a9ce6d840b53a6558038.jpeg\nSaved: ./kaggle/working/prediction/e4a17af18f72c8e6166a915669c99390.jpeg\nSaved: ./kaggle/working/prediction/0619ebebe9e9c9d00a4262b4fe4a5a95.jpeg\nSaved: ./kaggle/working/prediction/d3694abb47953b0e4909384b57bb6a05.jpeg\nSaved: ./kaggle/working/prediction/8b8ec74baddc22268d4b4ef4d95ceea1.jpeg\nSaved: ./kaggle/working/prediction/0626ab4ec3d46e602b296cc5cfd263f1.jpeg\nSaved: ./kaggle/working/prediction/633a8d5b2b2b55157b7781e2c706c75c.jpeg\nSaved: ./kaggle/working/prediction/be4d18d5401f659532897255ce2dd4ae.jpeg\nSaved: ./kaggle/working/prediction/ad43fe2cd066b9fdbc3bbc04a3afe1f1.jpeg\nSaved: ./kaggle/working/prediction/c5a0808bee60b246359c68c836f843dc.jpeg\nSaved: ./kaggle/working/prediction/6f67b5df7cdf3f33c3ca4d5060a633a8.jpeg\nSaved: ./kaggle/working/prediction/f13dd311a65d2b46d0a6085835c525af.jpeg\nSaved: ./kaggle/working/prediction/a6a4248a41e8db8b4ed633b456aaafac.jpeg\nSaved: ./kaggle/working/prediction/27738677a6b1f2c6d40b3bbba8f6c704.jpeg\nSaved: ./kaggle/working/prediction/268d4b4ef4d95ceea11957998906d369.jpeg\nSaved: ./kaggle/working/prediction/db5eb2a0e4b50889d874c68c030b9afe.jpeg\nSaved: ./kaggle/working/prediction/f8e5ad89d2844837f2a0f1536ad3f6a5.jpeg\nSaved: ./kaggle/working/prediction/6240619ebebe9e9c9d00a4262b4fe4a5.jpeg\nSaved: ./kaggle/working/prediction/4fda8daadc8dd23ae214d84b5dec33fd.jpeg\nSaved: ./kaggle/working/prediction/ff05dec1eb3a70b145a7d8d3b6c0ed75.jpeg\nSaved: ./kaggle/working/prediction/df366e057db382b8564872a27301a654.jpeg\nSaved: ./kaggle/working/prediction/425b976973f13dd311a65d2b46d0a608.jpeg\nSaved: ./kaggle/working/prediction/8954bb13d3727c7e5e1069646f2f0bb8.jpeg\nSaved: ./kaggle/working/prediction/dc70626ab4ec3d46e602b296cc5cfd26.jpeg\nSaved: ./kaggle/working/prediction/30c2f4fc276ed9f178dc2f4af6266509.jpeg\nSaved: ./kaggle/working/prediction/60a633a8d5b2b2b55157b7781e2c706c.jpeg\nSaved: ./kaggle/working/prediction/13dd311a65d2b46d0a6085835c525af6.jpeg\nSaved: ./kaggle/working/prediction/4f437f0019f7e6af7d7147763bdfb928.jpeg\nSaved: ./kaggle/working/prediction/c41545ba55aadaa77712a48e11d579d9.jpeg\nSaved: ./kaggle/working/prediction/e8bfb905b78a91391adc0bb223c4eaf3.jpeg\nSaved: ./kaggle/working/prediction/60b246359c68c836f843dcf41f4dce3c.jpeg\nSaved: ./kaggle/working/prediction/3b8318ecf467d7ad048df39beb176363.jpeg\nSaved: ./kaggle/working/prediction/8eb5a9a8a8d7fcc9df8e5ad89d284483.jpeg\nSaved: ./kaggle/working/prediction/05734fbeedd0f9da760db74a29abdb04.jpeg\nSaved: ./kaggle/working/prediction/5c1346e62522325c1b9c4fc9cbe1eca1.jpeg\nSaved: ./kaggle/working/prediction/80c643782707d7c359e27888daefee82.jpeg\nSaved: ./kaggle/working/prediction/5664c1711b62f15ec83b97bb11e8e0c4.jpeg\nSaved: ./kaggle/working/prediction/71f2fd85a04faeeb2b535797395305af.jpeg\nSaved: ./kaggle/working/prediction/4c1711b62f15ec83b97bb11e8e0c4416.jpeg\nSaved: ./kaggle/working/prediction/7b5df7cdf3f33c3ca4d5060a633a8d5b.jpeg\nSaved: ./kaggle/working/prediction/cf6644589e532a9ee954f81faedbce39.jpeg\nSaved: ./kaggle/working/prediction/5026b3550534bca540e24f489284b8e6.jpeg\nSaved: ./kaggle/working/prediction/dc0bb223c4eaf3372eae567c94ea04c6.jpeg\nSaved: ./kaggle/working/prediction/faef7fdb2d45b21960c94b0aab4c024a.jpeg\nSaved: ./kaggle/working/prediction/692195f853af7f8a4df1ec859759b7c8.jpeg\nSaved: ./kaggle/working/prediction/eb1ef57af2ed9fbb63b28163a745959c.jpeg\nSaved: ./kaggle/working/prediction/1531871f2fd85a04faeeb2b535797395.jpeg\nSaved: ./kaggle/working/prediction/e7998934d417cb2eb1ef57af2ed9fbb6.jpeg\nSaved: ./kaggle/working/prediction/782707d7c359e27888daefee82519763.jpeg\nSaved: ./kaggle/working/prediction/1c0e9082ea2c193ac8d551c149b60f29.jpeg\nSaved: ./kaggle/working/prediction/d6240619ebebe9e9c9d00a4262b4fe4a.jpeg\nSaved: ./kaggle/working/prediction/019410b1fcf0625f608b4ce97629ab55.jpeg\nSaved: ./kaggle/working/prediction/936de314f2d95e6c487ffa651b477422.jpeg\nSaved: ./kaggle/working/prediction/7330398846f67b5df7cdf3f33c3ca4d5.jpeg\nSaved: ./kaggle/working/prediction/998906d3694abb47953b0e4909384b57.jpeg\nSaved: ./kaggle/working/prediction/cc5cfd263f1f90be28799235026b3550.jpeg\nSaved: ./kaggle/working/prediction/285e26c90e1797c77826f9a7021bab9f.jpeg\nSaved: ./kaggle/working/prediction/98da48d679d7c7c8d3d96fb2b87fbbcf.jpeg\nSaved: ./kaggle/working/prediction/fe1f119f21b248d152b672ab3492fc62.jpeg\nSaved: ./kaggle/working/prediction/7f32574d6c748c41743c6c08a1d1ad8f.jpeg\nSaved: ./kaggle/working/prediction/50534bca540e24f489284b8e6953ad88.jpeg\nSaved: ./kaggle/working/prediction/9632a3c6f7f7fb2a643f15bd0249ddcc.jpeg\nSaved: ./kaggle/working/prediction/d5060a633a8d5b2b2b55157b7781e2c7.jpeg\nSaved: ./kaggle/working/prediction/eff05dec1eb3a70b145a7d8d3b6c0ed7.jpeg\nSaved: ./kaggle/working/prediction/395e56a6d9ba9d45c3dbc695325ded46.jpeg\nSaved: ./kaggle/working/prediction/39dda50f954ba59c7de13a35276a4764.jpeg\nSaved: ./kaggle/working/prediction/4baddc22268d4b4ef4d95ceea1195799.jpeg\nSaved: ./kaggle/working/prediction/7fda8019410b1fcf0625f608b4ce9762.jpeg\nSaved: ./kaggle/working/prediction/3425b976973f13dd311a65d2b46d0a60.jpeg\nSaved: ./kaggle/working/prediction/4e8bfb905b78a91391adc0bb223c4eaf.jpeg\nSaved: ./kaggle/working/prediction/39d6aad6bb0170a40ed32deef71fbe08.jpeg\nSaved: ./kaggle/working/prediction/fcd6da15fc656702fa602bb3c7abacdb.jpeg\nSaved: ./kaggle/working/prediction/15fc656702fa602bb3c7abacdbd7e6af.jpeg\nSaved: ./kaggle/working/prediction/5beb48f0be11d0309d1dff09b8405734.jpeg\nSaved: ./kaggle/working/prediction/41ed86e58224cb76a67d4dcf9596154e.jpeg\nSaved: ./kaggle/working/prediction/8395e56a6d9ba9d45c3dbc695325ded4.jpeg\nSaved: ./kaggle/working/prediction/cf464aa36bf7c09a3bb0e5ca159410b9.jpeg\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"# output the result\ndef rle_to_string(runs):\n    return ' '.join(str(x) for x in runs)\n\ndef rle_encode_one_mask(mask):\n    pixels = mask.flatten()\n    pixels[pixels > 225] = 255\n    pixels[pixels <= 225] = 0\n    use_padding = False\n    if pixels[0] or pixels[-1]:\n        use_padding = True\n        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n        pixel_padded[1:-1] = pixels\n        pixels = pixel_padded\n    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    if use_padding:\n        rle = rle - 1\n    rle[1::2] = rle[1::2] - rle[:-1:2]\n    \n    return rle_to_string(rle)\n\ndef rle2mask(mask_rle, shape=(3,3)):\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T\n\ndef mask2string(dir):\n    strings = []\n    ids = []\n    ws, hs = [[] for i in range(2)]\n    for image_id in os.listdir(dir):\n        id = image_id.split('.')[0]\n        path = os.path.join(dir, image_id)\n        print(path)\n        img = cv2.imread(path)[:,:,::-1]\n        h, w = img.shape[0], img.shape[1]\n        for channel in range(2):\n            ws.append(w)\n            hs.append(h)\n            ids.append(f'{id}_{channel}')\n            string = rle_encode_one_mask(img[:,:,channel])\n            strings.append(string)\n    r = {\n        'ids': ids,\n        'strings': strings,\n    }\n    return r\n\nMASK_DIR_PATH = '/kaggle/working/prediction'\ndir = MASK_DIR_PATH\nres = mask2string(dir)\ndf = pd.DataFrame(columns=['Id', 'Expected'])\ndf['Id'] = res['ids']\ndf['Expected'] = res['strings']\n\ndf.to_csv(r'/kaggle/working/output.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:39:06.049660Z","iopub.execute_input":"2024-11-24T16:39:06.049954Z","iopub.status.idle":"2024-11-24T16:39:07.596619Z","shell.execute_reply.started":"2024-11-24T16:39:06.049928Z","shell.execute_reply":"2024-11-24T16:39:07.595722Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/prediction/eff05dec1eb3a70b145a7d8d3b6c0ed7.jpeg\n/kaggle/working/prediction/cbb2a365b5574868eb60861ee1ff0b8a.jpeg\n/kaggle/working/prediction/4ca6160127cd1d5ff99c267599fc487b.jpeg\n/kaggle/working/prediction/db5eb2a0e4b50889d874c68c030b9afe.jpeg\n/kaggle/working/prediction/2a365b5574868eb60861ee1ff0b8a4f6.jpeg\n/kaggle/working/prediction/633a8d5b2b2b55157b7781e2c706c75c.jpeg\n/kaggle/working/prediction/7cb2eb1ef57af2ed9fbb63b28163a745.jpeg\n/kaggle/working/prediction/780fd497e1c0e9082ea2c193ac8d551c.jpeg\n/kaggle/working/prediction/fdbc3bbc04a3afe1f119f21b248d152b.jpeg\n/kaggle/working/prediction/66e057db382b8564872a27301a654864.jpeg\n/kaggle/working/prediction/0af3feff05dec1eb3a70b145a7d8d3b6.jpeg\n/kaggle/working/prediction/425b976973f13dd311a65d2b46d0a608.jpeg\n/kaggle/working/prediction/cf464aa36bf7c09a3bb0e5ca159410b9.jpeg\n/kaggle/working/prediction/c4be73749a0d21db70dd094a7f32574d.jpeg\n/kaggle/working/prediction/fb905b78a91391adc0bb223c4eaf3372.jpeg\n/kaggle/working/prediction/e4a17af18f72c8e6166a915669c99390.jpeg\n/kaggle/working/prediction/50534bca540e24f489284b8e6953ad88.jpeg\n/kaggle/working/prediction/94a7f32574d6c748c41743c6c08a1d1a.jpeg\n/kaggle/working/prediction/cb1b387133b51209db6dcdda5cc8a788.jpeg\n/kaggle/working/prediction/e7998934d417cb2eb1ef57af2ed9fbb6.jpeg\n/kaggle/working/prediction/461c2a337948a41964c1d4f50a5f3601.jpeg\n/kaggle/working/prediction/a9d45c3dbc695325ded465efde988dfb.jpeg\n/kaggle/working/prediction/ad43fe2cd066b9fdbc3bbc04a3afe1f1.jpeg\n/kaggle/working/prediction/1b62f15ec83b97bb11e8e0c4416c1931.jpeg\n/kaggle/working/prediction/0a5f3601ad4f13ccf1f4b331a412fc44.jpeg\n/kaggle/working/prediction/d6240619ebebe9e9c9d00a4262b4fe4a.jpeg\n/kaggle/working/prediction/1ad4f13ccf1f4b331a412fc44655fb51.jpeg\n/kaggle/working/prediction/7cdf3f33c3ca4d5060a633a8d5b2b2b5.jpeg\n/kaggle/working/prediction/391adc0bb223c4eaf3372eae567c94ea.jpeg\n/kaggle/working/prediction/41ed86e58224cb76a67d4dcf9596154e.jpeg\n/kaggle/working/prediction/72d9e593b6be1ac29adbe86f03d900fd.jpeg\n/kaggle/working/prediction/fcd6da15fc656702fa602bb3c7abacdb.jpeg\n/kaggle/working/prediction/cc5cfd263f1f90be28799235026b3550.jpeg\n/kaggle/working/prediction/6f67b5df7cdf3f33c3ca4d5060a633a8.jpeg\n/kaggle/working/prediction/30c2f4fc276ed9f178dc2f4af6266509.jpeg\n/kaggle/working/prediction/3c3ca4d5060a633a8d5b2b2b55157b77.jpeg\n/kaggle/working/prediction/13dd311a65d2b46d0a6085835c525af6.jpeg\n/kaggle/working/prediction/6ad1468996b4a9ce6d840b53a6558038.jpeg\n/kaggle/working/prediction/68d4b4ef4d95ceea11957998906d3694.jpeg\n/kaggle/working/prediction/a6a4248a41e8db8b4ed633b456aaafac.jpeg\n/kaggle/working/prediction/0fca6a4248a41e8db8b4ed633b456aaa.jpeg\n/kaggle/working/prediction/1531871f2fd85a04faeeb2b535797395.jpeg\n/kaggle/working/prediction/f13dd311a65d2b46d0a6085835c525af.jpeg\n/kaggle/working/prediction/7330398846f67b5df7cdf3f33c3ca4d5.jpeg\n/kaggle/working/prediction/e1797c77826f9a7021bab9fc73303988.jpeg\n/kaggle/working/prediction/6f4d4987ea3b4bae5672a230194c5a08.jpeg\n/kaggle/working/prediction/d6bf62f215f0da4ad3a7ab8df9da7386.jpeg\n/kaggle/working/prediction/5e8f14e1e0ae936de314f2d95e6c487f.jpeg\n/kaggle/working/prediction/3c692195f853af7f8a4df1ec859759b7.jpeg\n/kaggle/working/prediction/314fe384eb2ba3adfda6c1899fdc9837.jpeg\n/kaggle/working/prediction/26679bff55177a34fc01019eec999fd8.jpeg\n/kaggle/working/prediction/c41545ba55aadaa77712a48e11d579d9.jpeg\n/kaggle/working/prediction/a6e51d077bad31c8c5f54ffaa27a6235.jpeg\n/kaggle/working/prediction/7f32574d6c748c41743c6c08a1d1ad8f.jpeg\n/kaggle/working/prediction/dd094a7f32574d6c748c41743c6c08a1.jpeg\n/kaggle/working/prediction/dc0bb223c4eaf3372eae567c94ea04c6.jpeg\n/kaggle/working/prediction/4c1711b62f15ec83b97bb11e8e0c4416.jpeg\n/kaggle/working/prediction/3b8318ecf467d7ad048df39beb176363.jpeg\n/kaggle/working/prediction/3c84417fda8019410b1fcf0625f608b4.jpeg\n/kaggle/working/prediction/63b8318ecf467d7ad048df39beb17636.jpeg\n/kaggle/working/prediction/8fa8625605da2023387fd56c04414eaa.jpeg\n/kaggle/working/prediction/97e1c0e9082ea2c193ac8d551c149b60.jpeg\n/kaggle/working/prediction/f14e1e0ae936de314f2d95e6c487ffa6.jpeg\n/kaggle/working/prediction/e73749a0d21db70dd094a7f32574d6c7.jpeg\n/kaggle/working/prediction/a48847ae8395e56a6d9ba9d45c3dbc69.jpeg\n/kaggle/working/prediction/936de314f2d95e6c487ffa651b477422.jpeg\n/kaggle/working/prediction/5026b3550534bca540e24f489284b8e6.jpeg\n/kaggle/working/prediction/aafac813fe3ccba3e032dd2948a80c64.jpeg\n/kaggle/working/prediction/77e004e8bfb905b78a91391adc0bb223.jpeg\n/kaggle/working/prediction/cf6644589e532a9ee954f81faedbce39.jpeg\n/kaggle/working/prediction/c22268d4b4ef4d95ceea11957998906d.jpeg\n/kaggle/working/prediction/626650908b1cb932a767bf5487ced51b.jpeg\n/kaggle/working/prediction/5beb48f0be11d0309d1dff09b8405734.jpeg\n/kaggle/working/prediction/8954bb13d3727c7e5e1069646f2f0bb8.jpeg\n/kaggle/working/prediction/7b5df7cdf3f33c3ca4d5060a633a8d5b.jpeg\n/kaggle/working/prediction/ca4d5060a633a8d5b2b2b55157b7781e.jpeg\n/kaggle/working/prediction/60b246359c68c836f843dcf41f4dce3c.jpeg\n/kaggle/working/prediction/6231002ec4a1fe748f3085f1ce88cbdf.jpeg\n/kaggle/working/prediction/85a04faeeb2b535797395305af926a6f.jpeg\n/kaggle/working/prediction/c656702fa602bb3c7abacdbd7e6afd56.jpeg\n/kaggle/working/prediction/88e16d4ca6160127cd1d5ff99c267599.jpeg\n/kaggle/working/prediction/f62f215f0da4ad3a7ab8df9da7386835.jpeg\n/kaggle/working/prediction/285e26c90e1797c77826f9a7021bab9f.jpeg\n/kaggle/working/prediction/9632a3c6f7f7fb2a643f15bd0249ddcc.jpeg\n/kaggle/working/prediction/8395e56a6d9ba9d45c3dbc695325ded4.jpeg\n/kaggle/working/prediction/df366e057db382b8564872a27301a654.jpeg\n/kaggle/working/prediction/4e2a6e51d077bad31c8c5f54ffaa27a6.jpeg\n/kaggle/working/prediction/0626ab4ec3d46e602b296cc5cfd263f1.jpeg\n/kaggle/working/prediction/6ddca6ee1af35b65bd9ea42cfcfedb5e.jpeg\n/kaggle/working/prediction/9c7976c1182df0de51d32128c358d1fd.jpeg\n/kaggle/working/prediction/afe1f119f21b248d152b672ab3492fc6.jpeg\n/kaggle/working/prediction/eb1ef57af2ed9fbb63b28163a745959c.jpeg\n/kaggle/working/prediction/8eb5a9a8a8d7fcc9df8e5ad89d284483.jpeg\n/kaggle/working/prediction/a6d9ba9d45c3dbc695325ded465efde9.jpeg\n/kaggle/working/prediction/6240619ebebe9e9c9d00a4262b4fe4a5.jpeg\n/kaggle/working/prediction/625559c7e610b1531871f2fd85a04fae.jpeg\n/kaggle/working/prediction/be86f03d900fd197cd955fa095f97845.jpeg\n/kaggle/working/prediction/3425b976973f13dd311a65d2b46d0a60.jpeg\n/kaggle/working/prediction/677a6b1f2c6d40b3bbba8f6c704801b3.jpeg\n/kaggle/working/prediction/6d3694abb47953b0e4909384b57bb6a0.jpeg\n/kaggle/working/prediction/d694539ef2424a9218697283baa3657e.jpeg\n/kaggle/working/prediction/5664c1711b62f15ec83b97bb11e8e0c4.jpeg\n/kaggle/working/prediction/7fda8019410b1fcf0625f608b4ce9762.jpeg\n/kaggle/working/prediction/05734fbeedd0f9da760db74a29abdb04.jpeg\n/kaggle/working/prediction/1db239dda50f954ba59c7de13a35276a.jpeg\n/kaggle/working/prediction/5c1346e62522325c1b9c4fc9cbe1eca1.jpeg\n/kaggle/working/prediction/3657e4314fe384eb2ba3adfda6c1899f.jpeg\n/kaggle/working/prediction/dc70626ab4ec3d46e602b296cc5cfd26.jpeg\n/kaggle/working/prediction/4baddc22268d4b4ef4d95ceea1195799.jpeg\n/kaggle/working/prediction/268d4b4ef4d95ceea11957998906d369.jpeg\n/kaggle/working/prediction/67d4dcf9596154efb7cef748d9cbd617.jpeg\n/kaggle/working/prediction/4417fda8019410b1fcf0625f608b4ce9.jpeg\n/kaggle/working/prediction/559c7e610b1531871f2fd85a04faeeb2.jpeg\n/kaggle/working/prediction/ff05dec1eb3a70b145a7d8d3b6c0ed75.jpeg\n/kaggle/working/prediction/2ed9fbb63b28163a745959c03983064a.jpeg\n/kaggle/working/prediction/39dda50f954ba59c7de13a35276a4764.jpeg\n/kaggle/working/prediction/0619ebebe9e9c9d00a4262b4fe4a5a95.jpeg\n/kaggle/working/prediction/4ef4d95ceea11957998906d3694abb47.jpeg\n/kaggle/working/prediction/395e56a6d9ba9d45c3dbc695325ded46.jpeg\n/kaggle/working/prediction/998906d3694abb47953b0e4909384b57.jpeg\n/kaggle/working/prediction/f8e5ad89d2844837f2a0f1536ad3f6a5.jpeg\n/kaggle/working/prediction/3f33c3ca4d5060a633a8d5b2b2b55157.jpeg\n/kaggle/working/prediction/7f0019f7e6af7d7147763bdfb928d788.jpeg\n/kaggle/working/prediction/cb2eb1ef57af2ed9fbb63b28163a7459.jpeg\n/kaggle/working/prediction/d3694abb47953b0e4909384b57bb6a05.jpeg\n/kaggle/working/prediction/e2cd066b9fdbc3bbc04a3afe1f119f21.jpeg\n/kaggle/working/prediction/eecd70ebce6347c491b37c8c2e5a64a8.jpeg\n/kaggle/working/prediction/dd78294679c9cbb2a365b5574868eb60.jpeg\n/kaggle/working/prediction/87133b51209db6dcdda5cc8a788edaeb.jpeg\n/kaggle/working/prediction/9fc7330398846f67b5df7cdf3f33c3ca.jpeg\n/kaggle/working/prediction/80cae6daedd989517cb8041ed86e5822.jpeg\n/kaggle/working/prediction/0a0317371a966bf4b3466463a3c64db1.jpeg\n/kaggle/working/prediction/e5e8f14e1e0ae936de314f2d95e6c487.jpeg\n/kaggle/working/prediction/05b78a91391adc0bb223c4eaf3372eae.jpeg\n/kaggle/working/prediction/27738677a6b1f2c6d40b3bbba8f6c704.jpeg\n/kaggle/working/prediction/a3657e4314fe384eb2ba3adfda6c1899.jpeg\n/kaggle/working/prediction/71f2fd85a04faeeb2b535797395305af.jpeg\n/kaggle/working/prediction/aeeb2b535797395305af926a6f23c5d6.jpeg\n/kaggle/working/prediction/4e8bfb905b78a91391adc0bb223c4eaf.jpeg\n/kaggle/working/prediction/4fda8daadc8dd23ae214d84b5dec33fd.jpeg\n/kaggle/working/prediction/54ba59c7de13a35276a476420655433a.jpeg\n/kaggle/working/prediction/c5a0808bee60b246359c68c836f843dc.jpeg\n/kaggle/working/prediction/1c0e9082ea2c193ac8d551c149b60f29.jpeg\n/kaggle/working/prediction/3bbc04a3afe1f119f21b248d152b672a.jpeg\n/kaggle/working/prediction/c193ac8d551c149b60f2965341caf528.jpeg\n/kaggle/working/prediction/7af2ed9fbb63b28163a745959c039830.jpeg\n/kaggle/working/prediction/80c643782707d7c359e27888daefee82.jpeg\n/kaggle/working/prediction/c7e610b1531871f2fd85a04faeeb2b53.jpeg\n/kaggle/working/prediction/e56a6d9ba9d45c3dbc695325ded465ef.jpeg\n/kaggle/working/prediction/a51625559c7e610b1531871f2fd85a04.jpeg\n/kaggle/working/prediction/8cbdf366e057db382b8564872a27301a.jpeg\n/kaggle/working/prediction/af35b65bd9ea42cfcfedb5eb2a0e4b50.jpeg\n/kaggle/working/prediction/f7fdb2d45b21960c94b0aab4c024a573.jpeg\n/kaggle/working/prediction/d077bad31c8c5f54ffaa27a623511c38.jpeg\n/kaggle/working/prediction/6b83ef461c2a337948a41964c1d4f50a.jpeg\n/kaggle/working/prediction/343f27ebc5d92b9076135d76d0bbd4ce.jpeg\n/kaggle/working/prediction/ff55177a34fc01019eec999fd84e679b.jpeg\n/kaggle/working/prediction/be4d18d5401f659532897255ce2dd4ae.jpeg\n/kaggle/working/prediction/df8e26031fbb5e52c41545ba55aadaa7.jpeg\n/kaggle/working/prediction/6679bff55177a34fc01019eec999fd84.jpeg\n/kaggle/working/prediction/2cd066b9fdbc3bbc04a3afe1f119f21b.jpeg\n/kaggle/working/prediction/bec33b5e3d68f9d4c331587f9b9d49e2.jpeg\n/kaggle/working/prediction/0398846f67b5df7cdf3f33c3ca4d5060.jpeg\n/kaggle/working/prediction/1002ec4a1fe748f3085f1ce88cbdf366.jpeg\n/kaggle/working/prediction/a15fc656702fa602bb3c7abacdbd7e6a.jpeg\n/kaggle/working/prediction/5b21960c94b0aab4c024a573c692195f.jpeg\n/kaggle/working/prediction/c695325ded465efde988dfb96d081533.jpeg\n/kaggle/working/prediction/98da48d679d7c7c8d3d96fb2b87fbbcf.jpeg\n/kaggle/working/prediction/1209db6dcdda5cc8a788edaeb6aa460a.jpeg\n/kaggle/working/prediction/60a633a8d5b2b2b55157b7781e2c706c.jpeg\n/kaggle/working/prediction/e19769fa2d37d32780fd497e1c0e9082.jpeg\n/kaggle/working/prediction/ea42b4eebc9e5a87e443434ac60af150.jpeg\n/kaggle/working/prediction/b70dd094a7f32574d6c748c41743c6c0.jpeg\n/kaggle/working/prediction/4f437f0019f7e6af7d7147763bdfb928.jpeg\n/kaggle/working/prediction/d5060a633a8d5b2b2b55157b7781e2c7.jpeg\n/kaggle/working/prediction/710d568df17586ad8f3297c819c90895.jpeg\n/kaggle/working/prediction/e3c84417fda8019410b1fcf0625f608b.jpeg\n/kaggle/working/prediction/45b21960c94b0aab4c024a573c692195.jpeg\n/kaggle/working/prediction/7936140a2d5fc1443c4e445927738677.jpeg\n/kaggle/working/prediction/b21960c94b0aab4c024a573c692195f8.jpeg\n/kaggle/working/prediction/e9082ea2c193ac8d551c149b60f29653.jpeg\n/kaggle/working/prediction/fe1f119f21b248d152b672ab3492fc62.jpeg\n/kaggle/working/prediction/782707d7c359e27888daefee82519763.jpeg\n/kaggle/working/prediction/cdf3f33c3ca4d5060a633a8d5b2b2b55.jpeg\n/kaggle/working/prediction/8b8ec74baddc22268d4b4ef4d95ceea1.jpeg\n/kaggle/working/prediction/5a51625559c7e610b1531871f2fd85a0.jpeg\n/kaggle/working/prediction/7ad1cf2eb9d32a3dc907950289e976c7.jpeg\n/kaggle/working/prediction/15fc656702fa602bb3c7abacdbd7e6af.jpeg\n/kaggle/working/prediction/2d9e593b6be1ac29adbe86f03d900fd1.jpeg\n/kaggle/working/prediction/e8bfb905b78a91391adc0bb223c4eaf3.jpeg\n/kaggle/working/prediction/019410b1fcf0625f608b4ce97629ab55.jpeg\n/kaggle/working/prediction/f8e26031fbb5e52c41545ba55aadaa77.jpeg\n/kaggle/working/prediction/02fa602bb3c7abacdbd7e6afd56ea7bc.jpeg\n/kaggle/working/prediction/692195f853af7f8a4df1ec859759b7c8.jpeg\n/kaggle/working/prediction/82ea2c193ac8d551c149b60f2965341c.jpeg\n/kaggle/working/prediction/e1e0ae936de314f2d95e6c487ffa651b.jpeg\n/kaggle/working/prediction/3dd311a65d2b46d0a6085835c525af63.jpeg\n/kaggle/working/prediction/318ecf467d7ad048df39beb176363408.jpeg\n/kaggle/working/prediction/39d6aad6bb0170a40ed32deef71fbe08.jpeg\n/kaggle/working/prediction/faef7fdb2d45b21960c94b0aab4c024a.jpeg\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T15:43:22.993379Z","iopub.execute_input":"2024-11-24T15:43:22.993744Z","iopub.status.idle":"2024-11-24T15:43:23.001663Z","shell.execute_reply.started":"2024-11-24T15:43:22.993715Z","shell.execute_reply":"2024-11-24T15:43:23.000745Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"'/'"},"metadata":{}}],"execution_count":49}]}